//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35583870
// Cuda compilation tools, release 12.8, V12.8.93
// Based on NVVM 7.0.1
//

.version 8.7
.target sm_80
.address_size 64

	// .globl	_Z11main_kernelPvS_S_PmS0_
.func _ZN6kernel11linear_mainIN4type10bfloat16_tELi16ELi64ELi1024ELi6144ELi3EEEvPKvS4_S4_PvibPcbS6_S4_S4_
(
	.reg .b64 bruh_param_0,
	.reg .b64 bruh_param_1,
	.reg .b64 bruh_param_2,
	.reg .b64 bruh_param_3,
	.reg .b32 bruh_param_4,
	.reg .b64 bruh_param_5,
	.reg .b64 bruh_param_6,
	.reg .b64 bruh_param_7
)
;
.global .align 1 .b8 _ZN45_INTERNAL_e10375f7_14_many_linear_cu_cfd8d65b4cute7productE[1];
.global .align 1 .b8 _ZN45_INTERNAL_e10375f7_14_many_linear_cu_cfd8d65b4cute1_E[1];
.extern .shared .align 16 .b8 smem[];

.visible .entry _Z11main_kernelPvS_S_PmS0_(
	.param .u64 _Z11main_kernelPvS_S_PmS0__param_0,
	.param .u64 _Z11main_kernelPvS_S_PmS0__param_1,
	.param .u64 _Z11main_kernelPvS_S_PmS0__param_2,
	.param .u64 _Z11main_kernelPvS_S_PmS0__param_3,
	.param .u64 _Z11main_kernelPvS_S_PmS0__param_4
)
{
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<9>;
	.reg .b32 	%r<71>;
	.reg .b64 	%rd<48>;


	ld.param.u64 	%rd45, [_Z11main_kernelPvS_S_PmS0__param_0];
	ld.param.u64 	%rd35, [_Z11main_kernelPvS_S_PmS0__param_1];
	ld.param.u64 	%rd36, [_Z11main_kernelPvS_S_PmS0__param_2];
	// begin inline asm
	mov.u64 	%rd12, %clock64;
	// end inline asm
	mov.u32 	%r41, %tid.x;
	shr.s32 	%r42, %r41, 31;
	shr.u32 	%r43, %r42, 28;
	add.s32 	%r44, %r41, %r43;
	shr.s32 	%r45, %r44, 4;
	and.b32  	%r46, %r44, -16;
	shl.b32 	%r47, %r45, 10;
	sub.s32 	%r48, %r41, %r46;
	shl.b32 	%r49, %r48, 3;
	add.s32 	%r50, %r47, %r49;
	mul.wide.s32 	%rd37, %r50, 2;
	add.s64 	%rd13, %rd45, %rd37;
	and.b32  	%r51, %r49, 56;
	shl.b32 	%r52, %r45, 6;
	and.b32  	%r53, %r52, 448;
	or.b32  	%r54, %r53, %r51;
	shr.s32 	%r55, %r48, 31;
	shr.u32 	%r56, %r55, 29;
	add.s32 	%r57, %r48, %r56;
	shr.s32 	%r58, %r57, 3;
	shl.b32 	%r59, %r58, 10;
	shr.u32 	%r60, %r53, 3;
	xor.b32  	%r61, %r54, %r60;
	or.b32  	%r62, %r61, %r59;
	add.s64 	%rd15, %rd35, %rd37;
	mad.lo.s32 	%r63, %r58, 3072, %r62;
	setp.lt.s32 	%p1, %r41, 256;
	setp.lt.s32 	%p2, %r41, 128;
	selp.u16 	%rs1, 1, 0, %p2;
	mul.wide.u16 	%r4, %rs1, 16;
	setp.lt.s32 	%p3, %r41, 1024;
	setp.lt.s32 	%p4, %r41, 896;
	setp.lt.s32 	%p5, %r41, 768;
	setp.lt.s32 	%p6, %r41, 640;
	setp.lt.s32 	%p7, %r41, 512;
	selp.u16 	%rs2, 1, 0, %p7;
	mul.wide.u16 	%r14, %rs2, 16;
	setp.lt.s32 	%p8, %r41, 384;
	selp.u16 	%rs3, 1, 0, %p1;
	mul.wide.u16 	%r2, %rs3, 16;
	shl.b32 	%r64, %r62, 1;
	mov.u32 	%r65, smem;
	add.s32 	%r1, %r65, %r64;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r1], [%rd13], 16, %r2;

	// end inline asm
	add.s64 	%rd14, %rd13, 16384;
	add.s32 	%r3, %r1, 1024;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r3], [%rd14], 16, %r4;

	// end inline asm
	shl.b32 	%r66, %r63, 1;
	add.s32 	%r67, %r65, %r66;
	add.s32 	%r5, %r67, 12288;
	selp.u16 	%rs4, 1, 0, %p3;
	mul.wide.u16 	%r6, %rs4, 16;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r5], [%rd15], 16, %r6;

	// end inline asm
	add.s64 	%rd16, %rd15, 16384;
	add.s32 	%r7, %r67, 13312;
	selp.u16 	%rs5, 1, 0, %p4;
	mul.wide.u16 	%r8, %rs5, 16;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r7], [%rd16], 16, %r8;

	// end inline asm
	add.s64 	%rd17, %rd15, 32768;
	add.s32 	%r9, %r67, 14336;
	selp.u16 	%rs6, 1, 0, %p5;
	mul.wide.u16 	%r10, %rs6, 16;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r9], [%rd17], 16, %r10;

	// end inline asm
	add.s64 	%rd18, %rd15, 49152;
	add.s32 	%r11, %r67, 15360;
	selp.u16 	%rs7, 1, 0, %p6;
	mul.wide.u16 	%r12, %rs7, 16;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r11], [%rd18], 16, %r12;

	// end inline asm
	add.s64 	%rd19, %rd15, 65536;
	add.s32 	%r13, %r67, 16384;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r13], [%rd19], 16, %r14;

	// end inline asm
	add.s64 	%rd20, %rd15, 81920;
	add.s32 	%r15, %r67, 17408;
	selp.u16 	%rs8, 1, 0, %p8;
	mul.wide.u16 	%r16, %rs8, 16;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r15], [%rd20], 16, %r16;

	// end inline asm
	add.s64 	%rd21, %rd15, 98304;
	add.s32 	%r17, %r67, 18432;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r17], [%rd21], 16, %r2;

	// end inline asm
	add.s64 	%rd22, %rd15, 114688;
	add.s32 	%r19, %r67, 19456;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r19], [%rd22], 16, %r4;

	// end inline asm
	// begin inline asm
	cp.async.commit_group;

	// end inline asm
	add.s64 	%rd23, %rd13, 256;
	add.s32 	%r21, %r1, 4096;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r21], [%rd23], 16, %r2;

	// end inline asm
	add.s64 	%rd24, %rd13, 16640;
	add.s32 	%r23, %r1, 5120;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r23], [%rd24], 16, %r4;

	// end inline asm
	add.s64 	%rd25, %rd15, 256;
	add.s32 	%r25, %r67, 28672;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r25], [%rd25], 16, %r6;

	// end inline asm
	add.s64 	%rd26, %rd15, 16640;
	add.s32 	%r27, %r67, 29696;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r27], [%rd26], 16, %r8;

	// end inline asm
	add.s64 	%rd27, %rd15, 33024;
	add.s32 	%r29, %r67, 30720;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r29], [%rd27], 16, %r10;

	// end inline asm
	add.s64 	%rd28, %rd15, 49408;
	add.s32 	%r31, %r67, 31744;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r31], [%rd28], 16, %r12;

	// end inline asm
	add.s64 	%rd29, %rd15, 65792;
	add.s32 	%r33, %r67, 32768;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r33], [%rd29], 16, %r14;

	// end inline asm
	add.s64 	%rd30, %rd15, 82176;
	add.s32 	%r35, %r67, 33792;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r35], [%rd30], 16, %r16;

	// end inline asm
	add.s64 	%rd31, %rd15, 98560;
	add.s32 	%r37, %r67, 34816;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r37], [%rd31], 16, %r2;

	// end inline asm
	add.s64 	%rd32, %rd15, 114944;
	add.s32 	%r39, %r67, 35840;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r39], [%rd32], 16, %r4;

	// end inline asm
	// begin inline asm
	cp.async.commit_group;

	// end inline asm
	// begin inline asm
	mov.u64 	%rd33, %clock64;
	// end inline asm
	mov.u32 	%r68, %ctaid.x;
	mul.wide.u32 	%rd38, %r68, 128;
	add.s64 	%rd47, %rd36, %rd38;
	add.s64 	%rd44, %rd35, %rd38;
	mov.u64 	%rd46, 30;

$L__BB0_1:
	add.s64 	%rd39, %rd44, 12582912;
	add.s64 	%rd40, %rd45, 32768;
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r65;
	  cvta.shared.u64 	%rd41, %tmp; }
	add.s64 	%rd42, %rd41, 81920;
	{ // callseq 0, 0
	.reg .b32 temp_param_reg;
	call.uni 
	_ZN6kernel11linear_mainIN4type10bfloat16_tELi16ELi64ELi1024ELi6144ELi3EEEvPKvS4_S4_PvibPcbS6_S4_S4_, 
	(
	%rd45, 
	%rd44, 
	%rd47,
	%rd41, 
	1,
	%rd42,
	%rd40,
	%rd39
	);
	} // callseq 0
	setp.ne.s64 	%p9, %rd46, 2;
	add.s64 	%rd44, %rd44, 25165824;
	add.s64 	%rd45, %rd45, 65536;
	add.s64 	%rd43, %rd47, 196608;
	selp.u32 	%r70, 1, 0, %p9;
	{ // callseq 1, 0
	.reg .b32 temp_param_reg;
	call.uni 
	_ZN6kernel11linear_mainIN4type10bfloat16_tELi16ELi64ELi1024ELi6144ELi3EEEvPKvS4_S4_PvibPcbS6_S4_S4_, 
	(
	%rd40, 
	%rd39, 
	%rd43, 
	%rd42, 
	%r70,
	%rd41,
	%rd45,
	%rd44
	);
	} // callseq 1
	add.s64 	%rd47, %rd47, 393216;
	add.s64 	%rd46, %rd46, -2;
	setp.ne.s64 	%p10, %rd46, 0;
	@%p10 bra 	$L__BB0_1;

	ret;

}
.func _ZN6kernel11linear_mainIN4type10bfloat16_tELi16ELi64ELi1024ELi6144ELi3EEEvPKvS4_S4_PvibPcbS6_S4_S4_(
	.reg .b64 bruh_param_0,
	.reg .b64 bruh_param_1,
	.reg .b64 bruh_param_2,
	.reg .b64 bruh_param_3,
	.reg .b32 bruh_param_4,
	.reg .b64 bruh_param_5,
	.reg .b64 bruh_param_6,
	.reg .b64 bruh_param_7
)
{
	.reg .pred 	%p<18>;
	.reg .b16 	%rs<19>;
	.reg .f32 	%f<169>;
	.reg .b32 	%r<484>;
	.reg .b64 	%rd<58>;


	{ .reg .b64 %tmp;
	  cvta.to.shared.u64 	%tmp, bruh_param_3;
	  cvt.u32.u64 	%r1, %tmp; }
	mov.u32 	%r2, %tid.x;
	shr.s32 	%r85, %r2, 31;
	shr.u32 	%r86, %r85, 28;
	add.s32 	%r87, %r2, %r86;
	and.b32  	%r88, %r87, -16;
	sub.s32 	%r89, %r2, %r88;
	shr.s32 	%r90, %r89, 31;
	shr.u32 	%r91, %r90, 29;
	add.s32 	%r92, %r89, %r91;
	and.b32  	%r93, %r92, -8;
	sub.s32 	%r94, %r89, %r93;
	shr.u32 	%r95, %r87, 31;
	shr.s32 	%r96, %r87, 4;
	add.s32 	%r97, %r96, %r95;
	and.b32  	%r98, %r97, -2;
	sub.s32 	%r99, %r96, %r98;
	mov.u32 	%r483, 2;
	shl.b32 	%r100, %r94, 6;
	and.b32  	%r101, %r100, 448;
	shl.b32 	%r102, %r99, 3;
	and.b32  	%r103, %r102, 8;
	or.b32  	%r104, %r103, %r101;
	shr.s32 	%r105, %r92, 3;
	shl.b32 	%r106, %r105, 9;
	and.b32  	%r107, %r94, 2;
	setp.eq.s32 	%p1, %r107, 0;
	selp.b32 	%r3, 16, -16, %p1;
	and.b32  	%r108, %r94, 4;
	setp.eq.s32 	%p2, %r108, 0;
	selp.b32 	%r4, 32, -32, %p2;
	shr.u32 	%r109, %r101, 3;
	xor.b32  	%r110, %r104, %r109;
	or.b32  	%r111, %r110, %r106;
	shr.u32 	%r112, %r85, 29;
	add.s32 	%r113, %r2, %r112;
	and.b32  	%r114, %r113, -8;
	sub.s32 	%r6, %r2, %r114;
	mov.u32 	%r478, 0;
	shl.b32 	%r115, %r6, 6;
	and.b32  	%r116, %r115, 448;
	and.b32  	%r117, %r113, 8;
	or.b32  	%r118, %r117, %r116;
	shr.u32 	%r119, %r85, 27;
	add.s32 	%r120, %r2, %r119;
	shr.s32 	%r7, %r120, 5;
	shl.b32 	%r121, %r99, 11;
	shl.b32 	%r122, %r7, 9;
	add.s32 	%r123, %r121, %r122;
	and.b32  	%r124, %r6, 2;
	setp.eq.s32 	%p3, %r124, 0;
	selp.b32 	%r8, 16, -16, %p3;
	and.b32  	%r125, %r6, 4;
	setp.eq.s32 	%p4, %r125, 0;
	selp.b32 	%r9, 32, -32, %p4;
	shr.u32 	%r126, %r116, 3;
	xor.b32  	%r127, %r118, %r126;
	or.b32  	%r128, %r123, %r127;
	shl.b32 	%r129, %r89, 3;
	shl.b32 	%r130, %r96, 10;
	add.s32 	%r131, %r130, %r129;
	cvt.s64.s32 	%rd2, %r131;
	shl.b32 	%r132, %r94, 3;
	and.b32  	%r133, %r132, 56;
	shl.b32 	%r134, %r96, 6;
	and.b32  	%r135, %r134, 448;
	or.b32  	%r136, %r135, %r133;
	shl.b32 	%r137, %r105, 10;
	shr.u32 	%r138, %r135, 3;
	xor.b32  	%r139, %r136, %r138;
	or.b32  	%r10, %r139, %r137;
	mad.lo.s32 	%r11, %r105, 3072, %r10;
	setp.lt.s32 	%p5, %r2, 256;
	setp.lt.s32 	%p6, %r2, 128;
	selp.u16 	%rs2, 1, 0, %p6;
	mul.wide.u16 	%r12, %rs2, 16;
	setp.lt.s32 	%p7, %r2, 1024;
	setp.lt.s32 	%p8, %r2, 896;
	setp.lt.s32 	%p9, %r2, 768;
	setp.lt.s32 	%p10, %r2, 640;
	setp.lt.s32 	%p11, %r2, 512;
	selp.u16 	%rs3, 1, 0, %p11;
	mul.wide.u16 	%r13, %rs3, 16;
	setp.lt.s32 	%p12, %r2, 384;
	selp.u16 	%rs4, 1, 0, %p5;
	mul.wide.u16 	%r14, %rs4, 16;
	// begin inline asm
	cp.async.wait_group 1;

	// end inline asm
	bar.sync 	0;
	shl.b32 	%r140, %r111, 1;
	add.s32 	%r75, %r1, %r140;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r473, %r472, %r471, %r470}, [%r75];

	// end inline asm
	add.s32 	%r20, %r1, 12288;
	shl.b32 	%r141, %r128, 1;
	add.s32 	%r80, %r20, %r141;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r477, %r476, %r475, %r474}, [%r80];

	// end inline asm
	selp.u16 	%rs5, 1, 0, %p7;
	mul.wide.u16 	%r26, %rs5, 16;
	selp.u16 	%rs6, 1, 0, %p8;
	mul.wide.u16 	%r27, %rs6, 16;
	selp.u16 	%rs7, 1, 0, %p9;
	mul.wide.u16 	%r28, %rs7, 16;
	selp.u16 	%rs8, 1, 0, %p10;
	mul.wide.u16 	%r29, %rs8, 16;
	selp.u16 	%rs9, 1, 0, %p12;
	mul.wide.u16 	%r30, %rs9, 16;
	mov.f32 	%f161, 0f00000000;
	mov.f32 	%f162, %f161;
	mov.f32 	%f163, %f161;
	mov.f32 	%f164, %f161;
	mov.f32 	%f165, %f161;
	mov.f32 	%f166, %f161;
	mov.f32 	%f167, %f161;
	mov.f32 	%f168, %f161;
	mov.u32 	%r480, %r478;
	mov.u32 	%r482, %r483;

$L__BB1_1:
	.pragma "nounroll";
	shl.b32 	%r43, %r480, 11;
	add.s32 	%r152, %r43, %r3;
	shl.b32 	%r153, %r152, 1;
	add.s32 	%r146, %r75, %r153;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r142, %r143, %r144, %r145}, [%r146];

	// end inline asm
	shl.b32 	%r49, %r480, 13;
	add.s32 	%r154, %r49, %r8;
	shl.b32 	%r155, %r154, 1;
	add.s32 	%r151, %r80, %r155;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r147, %r148, %r149, %r150}, [%r151];

	// end inline asm
	setp.gt.s32 	%p13, %r483, 7;
	@%p13 bra 	$L__BB1_3;

	shl.b32 	%r176, %r483, 7;
	cvt.s64.s32 	%rd21, %r176;
	shl.b32 	%r177, %r482, 11;
	add.s32 	%r178, %r10, %r177;
	add.s64 	%rd22, %rd2, %rd21;
	shl.b64 	%rd23, %rd22, 1;
	add.s64 	%rd11, bruh_param_0, %rd23;
	shl.b32 	%r179, %r178, 1;
	add.s32 	%r156, %r1, %r179;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r156], [%rd11], 16, %r14;

	// end inline asm
	add.s64 	%rd12, %rd11, 16384;
	add.s32 	%r158, %r156, 1024;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r158], [%rd12], 16, %r12;

	// end inline asm
	shl.b32 	%r180, %r482, 13;
	add.s32 	%r181, %r11, %r180;
	add.s64 	%rd13, bruh_param_1, %rd23;
	shl.b32 	%r182, %r181, 1;
	add.s32 	%r160, %r20, %r182;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r160], [%rd13], 16, %r26;

	// end inline asm
	add.s64 	%rd14, %rd13, 16384;
	add.s32 	%r162, %r160, 1024;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r162], [%rd14], 16, %r27;

	// end inline asm
	add.s64 	%rd15, %rd13, 32768;
	add.s32 	%r164, %r160, 2048;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r164], [%rd15], 16, %r28;

	// end inline asm
	add.s64 	%rd16, %rd13, 49152;
	add.s32 	%r166, %r160, 3072;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r166], [%rd16], 16, %r29;

	// end inline asm
	add.s64 	%rd17, %rd13, 65536;
	add.s32 	%r168, %r160, 4096;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r168], [%rd17], 16, %r13;

	// end inline asm
	add.s64 	%rd18, %rd13, 81920;
	add.s32 	%r170, %r160, 5120;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r170], [%rd18], 16, %r30;

	// end inline asm
	add.s64 	%rd19, %rd13, 98304;
	add.s32 	%r172, %r160, 6144;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r172], [%rd19], 16, %r14;

	// end inline asm
	add.s64 	%rd20, %rd13, 114688;
	add.s32 	%r174, %r160, 7168;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r174], [%rd20], 16, %r12;

	// end inline asm
	add.s32 	%r483, %r483, 1;
	add.s32 	%r183, %r482, 1;
	mul.hi.s32 	%r184, %r183, 1431655766;
	shr.u32 	%r185, %r184, 31;
	add.s32 	%r186, %r184, %r185;
	mul.lo.s32 	%r187, %r186, 3;
	sub.s32 	%r482, %r183, %r187;

$L__BB1_3:
	// begin inline asm
	cp.async.commit_group;

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {%f25,  %f26,  %f27,  %f28},{%r473,  %r472,  %r471,  %r470},{%r477,  %r476},{%f168, %f167, %f166, %f165};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {%f33,  %f34,  %f35,  %f36},{%r473,  %r472,  %r471,  %r470},{%r475,  %r474},{%f164, %f163, %f162, %f161};

	// end inline asm
	add.s32 	%r354, %r43, %r4;
	shl.b32 	%r355, %r354, 1;
	add.s32 	%r204, %r75, %r355;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r200, %r201, %r202, %r203}, [%r204];

	// end inline asm
	add.s32 	%r356, %r49, %r9;
	shl.b32 	%r357, %r356, 1;
	add.s32 	%r209, %r80, %r357;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r205, %r206, %r207, %r208}, [%r209];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {%f41,  %f42,  %f43,  %f44},{%r142,  %r143,  %r144,  %r145},{%r147,  %r148},{%f25, %f26, %f27, %f28};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {%f49,  %f50,  %f51,  %f52},{%r142,  %r143,  %r144,  %r145},{%r149,  %r150},{%f33, %f34, %f35, %f36};

	// end inline asm
	shl.b32 	%r358, %r4, 1;
	add.s32 	%r226, %r146, %r358;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r222, %r223, %r224, %r225}, [%r226];

	// end inline asm
	shl.b32 	%r359, %r9, 1;
	add.s32 	%r231, %r151, %r359;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r227, %r228, %r229, %r230}, [%r231];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {%f57,  %f58,  %f59,  %f60},{%r200,  %r201,  %r202,  %r203},{%r205,  %r206},{%f41, %f42, %f43, %f44};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {%f65,  %f66,  %f67,  %f68},{%r200,  %r201,  %r202,  %r203},{%r207,  %r208},{%f49, %f50, %f51, %f52};

	// end inline asm
	shl.b32 	%r360, %r43, 1;
	add.s32 	%r361, %r75, %r360;
	add.s32 	%r248, %r361, 2048;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r244, %r245, %r246, %r247}, [%r248];

	// end inline asm
	shl.b32 	%r362, %r49, 1;
	add.s32 	%r363, %r80, %r362;
	add.s32 	%r253, %r363, 8192;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r249, %r250, %r251, %r252}, [%r253];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {%f73,  %f74,  %f75,  %f76},{%r222,  %r223,  %r224,  %r225},{%r227,  %r228},{%f57, %f58, %f59, %f60};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {%f81,  %f82,  %f83,  %f84},{%r222,  %r223,  %r224,  %r225},{%r229,  %r230},{%f65, %f66, %f67, %f68};

	// end inline asm
	shl.b32 	%r364, %r3, 1;
	add.s32 	%r270, %r248, %r364;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r266, %r267, %r268, %r269}, [%r270];

	// end inline asm
	shl.b32 	%r365, %r8, 1;
	add.s32 	%r275, %r253, %r365;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r271, %r272, %r273, %r274}, [%r275];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {%f89,  %f90,  %f91,  %f92},{%r244,  %r245,  %r246,  %r247},{%r249,  %r250},{%f73, %f74, %f75, %f76};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {%f97,  %f98,  %f99,  %f100},{%r244,  %r245,  %r246,  %r247},{%r251,  %r252},{%f81, %f82, %f83, %f84};

	// end inline asm
	add.s32 	%r292, %r248, %r358;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r288, %r289, %r290, %r291}, [%r292];

	// end inline asm
	add.s32 	%r297, %r253, %r359;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r293, %r294, %r295, %r296}, [%r297];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {%f105,  %f106,  %f107,  %f108},{%r266,  %r267,  %r268,  %r269},{%r271,  %r272},{%f89, %f90, %f91, %f92};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {%f113,  %f114,  %f115,  %f116},{%r266,  %r267,  %r268,  %r269},{%r273,  %r274},{%f97, %f98, %f99, %f100};

	// end inline asm
	add.s32 	%r314, %r270, %r358;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r310, %r311, %r312, %r313}, [%r314];

	// end inline asm
	add.s32 	%r319, %r275, %r359;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r315, %r316, %r317, %r318}, [%r319];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {%f121,  %f122,  %f123,  %f124},{%r288,  %r289,  %r290,  %r291},{%r293,  %r294},{%f105, %f106, %f107, %f108};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {%f129,  %f130,  %f131,  %f132},{%r288,  %r289,  %r290,  %r291},{%r295,  %r296},{%f113, %f114, %f115, %f116};

	// end inline asm
	// begin inline asm
	cp.async.wait_group 1;

	// end inline asm
	bar.sync 	0;
	add.s32 	%r366, %r480, 1;
	mul.hi.s32 	%r367, %r366, 1431655766;
	shr.u32 	%r368, %r367, 31;
	add.s32 	%r369, %r367, %r368;
	mul.lo.s32 	%r370, %r369, 3;
	sub.s32 	%r480, %r366, %r370;
	shl.b32 	%r60, %r480, 11;
	shl.b32 	%r371, %r480, 12;
	add.s32 	%r336, %r75, %r371;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r473, %r472, %r471, %r470}, [%r336];

	// end inline asm
	shl.b32 	%r372, %r480, 14;
	add.s32 	%r341, %r80, %r372;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r477, %r476, %r475, %r474}, [%r341];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {%f168,  %f167,  %f166,  %f165},{%r310,  %r311,  %r312,  %r313},{%r315,  %r316},{%f121, %f122, %f123, %f124};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {%f164,  %f163,  %f162,  %f161},{%r310,  %r311,  %r312,  %r313},{%r317,  %r318},{%f129, %f130, %f131, %f132};

	// end inline asm
	add.s32 	%r478, %r478, 1;
	setp.ne.s32 	%p14, %r478, 8;
	@%p14 bra 	$L__BB1_1;

	cvt.s8.s32      %rs18, bruh_param_4;
	setp.eq.s16 	%p15, %rs18, 0;
	@%p15 bra 	$L__BB1_6;

	shl.b64 	%rd44, %rd2, 1;
	add.s64 	%rd24, bruh_param_6, %rd44;
	add.s64 	%rd26, bruh_param_7, %rd44;
	{ .reg .b64 %tmp;
	  cvta.to.shared.u64 	%tmp, bruh_param_5;
	  cvt.u32.u64 	%r413, %tmp; }
	shl.b32 	%r414, %r11, 1;
	add.s32 	%r415, %r413, %r414;
	shl.b32 	%r416, %r10, 1;
	add.s32 	%r373, %r413, %r416;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r373], [%rd24], 16, %r14;

	// end inline asm
	add.s64 	%rd25, %rd24, 16384;
	add.s32 	%r375, %r373, 1024;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r375], [%rd25], 16, %r12;

	// end inline asm
	add.s32 	%r377, %r415, 12288;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r377], [%rd26], 16, %r26;

	// end inline asm
	add.s64 	%rd27, %rd26, 16384;
	add.s32 	%r379, %r415, 13312;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r379], [%rd27], 16, %r27;

	// end inline asm
	add.s64 	%rd28, %rd26, 32768;
	add.s32 	%r381, %r415, 14336;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r381], [%rd28], 16, %r28;

	// end inline asm
	add.s64 	%rd29, %rd26, 49152;
	add.s32 	%r383, %r415, 15360;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r383], [%rd29], 16, %r29;

	// end inline asm
	add.s64 	%rd30, %rd26, 65536;
	add.s32 	%r385, %r415, 16384;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r385], [%rd30], 16, %r13;

	// end inline asm
	add.s64 	%rd31, %rd26, 81920;
	add.s32 	%r387, %r415, 17408;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r387], [%rd31], 16, %r30;

	// end inline asm
	add.s64 	%rd32, %rd26, 98304;
	add.s32 	%r389, %r415, 18432;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r389], [%rd32], 16, %r14;

	// end inline asm
	add.s64 	%rd33, %rd26, 114688;
	add.s32 	%r391, %r415, 19456;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r391], [%rd33], 16, %r12;

	// end inline asm
	// begin inline asm
	cp.async.commit_group;

	// end inline asm
	add.s64 	%rd34, %rd24, 256;
	add.s32 	%r393, %r373, 4096;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r393], [%rd34], 16, %r14;

	// end inline asm
	add.s64 	%rd35, %rd24, 16640;
	add.s32 	%r395, %r373, 5120;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r395], [%rd35], 16, %r12;

	// end inline asm
	add.s64 	%rd36, %rd26, 256;
	add.s32 	%r397, %r415, 28672;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r397], [%rd36], 16, %r26;

	// end inline asm
	add.s64 	%rd37, %rd26, 16640;
	add.s32 	%r399, %r415, 29696;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r399], [%rd37], 16, %r27;

	// end inline asm
	add.s64 	%rd38, %rd26, 33024;
	add.s32 	%r401, %r415, 30720;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r401], [%rd38], 16, %r28;

	// end inline asm
	add.s64 	%rd39, %rd26, 49408;
	add.s32 	%r403, %r415, 31744;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r403], [%rd39], 16, %r29;

	// end inline asm
	add.s64 	%rd40, %rd26, 65792;
	add.s32 	%r405, %r415, 32768;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r405], [%rd40], 16, %r13;

	// end inline asm
	add.s64 	%rd41, %rd26, 82176;
	add.s32 	%r407, %r415, 33792;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r407], [%rd41], 16, %r30;

	// end inline asm
	add.s64 	%rd42, %rd26, 98560;
	add.s32 	%r409, %r415, 34816;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r409], [%rd42], 16, %r14;

	// end inline asm
	add.s64 	%rd43, %rd26, 114944;
	add.s32 	%r411, %r415, 35840;
	// begin inline asm
	cp.async.cg.shared.global.L2::128B [%r411], [%rd43], 16, %r12;

	// end inline asm
	// begin inline asm
	cp.async.commit_group;

	// end inline asm

$L__BB1_6:
	mov.u32 	%r469, %tid.x;
	shr.s32 	%r468, %r469, 31;
	shr.u32 	%r467, %r468, 29;
	add.s32 	%r466, %r469, %r467;
	shr.s32 	%r465, %r466, 3;
	and.b32  	%r464, %r466, -8;
	sub.s32 	%r463, %r469, %r464;
	shr.u32 	%r462, %r468, 27;
	add.s32 	%r461, %r469, %r462;
	shr.s32 	%r460, %r461, 5;
	mov.u32 	%r459, %tid.x;
	cvta.to.global.u64 	%rd56, bruh_param_2;
	shr.s32 	%r458, %r459, 31;
	setp.gt.s32 	%p16, %r459, 127;
	shr.u32 	%r418, %r458, 30;
	add.s32 	%r419, %r459, %r418;
	shr.s32 	%r420, %r419, 2;
	shr.s32 	%r421, %r419, 31;
	shr.u32 	%r422, %r421, 29;
	add.s32 	%r423, %r420, %r422;
	and.b32  	%r424, %r423, -8;
	sub.s32 	%r425, %r420, %r424;
	shl.b32 	%r426, %r425, 6;
	and.b32  	%r427, %r426, 448;
	shl.b32 	%r428, %r460, 3;
	and.b32  	%r429, %r428, 24;
	or.b32  	%r430, %r429, %r427;
	and.b32  	%r431, %r419, 2147483644;
	sub.s32 	%r432, %r459, %r431;
	shl.b32 	%r433, %r432, 1;
	and.b32  	%r434, %r425, 4;
	setp.eq.s32 	%p17, %r434, 0;
	shr.u32 	%r435, %r427, 3;
	xor.b32  	%r436, %r430, %r435;
	add.s32 	%r437, %r436, %r433;
	shl.b32 	%r438, %r463, 3;
	and.b32  	%r439, %r438, 56;
	shl.b32 	%r440, %r465, 6;
	and.b32  	%r441, %r440, 448;
	or.b32  	%r442, %r441, %r439;
	shr.u32 	%r443, %r458, 26;
	add.s32 	%r444, %r459, %r443;
	shl.b32 	%r445, %r444, 3;
	and.b32  	%r446, %r445, 2147483136;
	shr.u32 	%r447, %r441, 3;
	xor.b32  	%r448, %r442, %r447;
	or.b32  	%r449, %r448, %r446;
	add.s32 	%r450, %r449, %r60;
	mad.lo.s32 	%r451, %r465, 6144, %r438;
	cvt.u64.u32 	%rd45, %r437;
	cvt.u64.u32 	%rd46, %r60;
	add.s64 	%rd47, %rd45, %rd46;
	shl.b32 	%r452, %r450, 1;
	add.s32 	%r70, %r1, %r452;
	mul.wide.s32 	%rd48, %r451, 2;
	add.s64 	%rd3, %rd56, %rd48;
	// begin inline asm
	cvt.rn.bf16.f32 %rs11, %f167;

	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs10, %f168;

	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs13, %f165;

	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs12, %f166;

	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs15, %f163;

	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs14, %f164;

	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs17, %f161;

	// end inline asm
	// begin inline asm
	cvt.rn.bf16.f32 %rs16, %f162;

	// end inline asm
	cvt.u32.u64 	%r453, %rd47;
	shl.b32 	%r454, %r453, 1;
	add.s32 	%r455, %r1, %r454;
	st.shared.v2.u16 	[%r455], {%rs10, %rs11};
	st.shared.v2.u16 	[%r455+1024], {%rs12, %rs13};
	selp.b32 	%r456, 64, -64, %p17;
	add.s32 	%r457, %r455, %r456;
	st.shared.v2.u16 	[%r457], {%rs14, %rs15};
	st.shared.v2.u16 	[%r457+1024], {%rs16, %rs17};
	bar.sync 	0;
	@%p16 bra 	$L__BB1_8;

	ld.shared.v2.u64 	{%rd49, %rd50}, [%r70];
	st.global.v2.u64 	[%rd3], {%rd49, %rd50};

$L__BB1_8:
	ret;

}

